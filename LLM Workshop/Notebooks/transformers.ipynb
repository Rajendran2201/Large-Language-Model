{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajendran/Desktop/LLM/llm_workshop/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing the dependencies , we are importing three models and torch which is used to encode the string / text\n",
    "# torch package helps to convert string into numeric data of vectors \n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-regressive Transformer (GPT-like model) - generate new text \n",
    "def generate_text(prompt):\n",
    "     # tokenizer - creates a layer helps in understanding the emotions of the prompt given by the user \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # generate - It is used to generate new texts \n",
    "    # num_return_sequence is used for the purpose of finetuning\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    # decode() is used to decode / convert the numeric data into string \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# BERT-like model for generate_text and translate_text\n",
    "def bert_generate_text(prompt):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def bart_generate_text(prompt):\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text GPT : Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text BERT : once upon a time, yes... but... but... but... but but... but... but... but... but... but... but... but... but... but... but... but... but... but...... but... but...... but......... but...... but... but........ but... but... but... but... but...... but... but.. but.............................\n",
      "Generated Text BART: Once upon a time,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nhallucination : \\n\\n- \"hallucination\" refers to the generation of text that appears to be \\n  coherent and contextually relevant but is not grounded in actual knowledge or facts. \\n- Hallucination can occur when a language model generates information that is not present in its training data or\\n  when it combines existing knowledge in unrealistic or improbable ways.\\n-  GPT model is better than bard. Why ?\\n-  GPT can form the sentence based on the given prompt, but BARD cannot do it \\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Once upon a time,\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"Generated Text GPT :\", generated_text)\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "generated_text = bert_generate_text(prompt)\n",
    "print(\"Generated Text BERT :\", generated_text)\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "generated_text = bart_generate_text(prompt)\n",
    "print(\"Generated Text BART:\", generated_text)\n",
    "\n",
    "''' \n",
    "hallucination : \n",
    "\n",
    "- \"hallucination\" refers to the generation of text that appears to be \n",
    "  coherent and contextually relevant but is not grounded in actual knowledge or facts. \n",
    "- Hallucination can occur when a language model generates information that is not present in its training data or\n",
    "  when it combines existing knowledge in unrealistic or improbable ways.\n",
    "-  GPT model is better than bard. Why ?\n",
    "-  GPT can form the sentence based on the given prompt, but BARD cannot do it \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-encoding Transformer (GPT-like model)\n",
    "\n",
    "def bert_auto_encoding(sentence):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    output = model(input_ids)\n",
    "    logits = output.logits\n",
    "    masked_logits = logits[0, masked_index, :]\n",
    "    predicted_token_ids = torch.argmax(masked_logits, dim=-1)\n",
    "    predicted_tokens = tokenizer.decode(predicted_token_ids)\n",
    "    return predicted_tokens\n",
    "\n",
    "def bart_auto_encoding(sentence):\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    input_text = sentence.replace(\"[MASK]\", tokenizer.mask_token)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    labels = input_ids.clone()\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    predicted_token_id = torch.argmax(outputs.logits[0, -2]).item()  # Get the token before the last token\n",
    "    predicted_word = tokenizer.decode([predicted_token_id])\n",
    "    return loss.item(), predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Predicted Word: buy\n",
      "BART Auto-Encoding Loss: 1.6794246435165405 Predicted Word: .\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence = \"I want to [MASK] a car.\"\n",
    "\n",
    "bert_predicted_word = bert_auto_encoding(sentence)\n",
    "print(\"BERT Predicted Word:\", bert_predicted_word)\n",
    "\n",
    "bart_loss, bart_predicted_word = bart_auto_encoding(sentence)\n",
    "print(\"BART Auto-Encoding Loss:\", bart_loss, \"Predicted Word:\", bart_predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text (french) : Je m'appelle Raj et j'étudie la première année, B.tech AI & DS à l'Institut de technologie de Coimbatore.\n"
     ]
    }
   ],
   "source": [
    "## Sentence to Sentence Transformers\n",
    "# We are going to translate the text from english to french \n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import sentencepiece\n",
    "\n",
    "def translate_to_french(input_text):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Translate the input text to French\n",
    "    translated_ids = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    # Decode the translated text\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"My name is Raj and I am studying first year,B.tech AI & DS in Coimbatore Institute of Technology.\"\n",
    "translated_text = translate_to_french(input_text)\n",
    "print(\"Translated Text (french) :\", translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
